[net]
# Training
batch=120
subdivisions=4
# Testing
#batch=1
#subdivisions=1
height=224
width=224
channels=3
momentum=0.9
decay=0.0005
max_crop=256

burn_in=1000
#burn_in=100
learning_rate=0.256
policy=poly
power=4
max_batches=800000
momentum=0.9
decay=0.00005

angle=7
hue=.1
saturation=.75
exposure=.75
aspect=.75

###### STEM - downsample
# conv1
[convolutional]
filters=24
size=3
pad=1
stride=2
batch_normalize=1
activation=swish


######################
###### 'r1_k3_a1_p1_s11_e1_i24_o24'
###### 3x3 - 24

# dw
[convolutional]
filters=24
groups=6
size=3
stride=1
pad=1
batch_normalize=1
activation=leaky

# out
[convolutional]
filters=24
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

[shortcut]
from=-3
activation=linear


######################
###### 'r1_k3.5.7_a1.1_p1.1_s22_e6_i24_o32',
###### 3x3, 5x5, 7x7 - 32 - downsample
###### expand 6 = 144

# expand
[convolutional]
filters=144
groups=2
size=1
stride=1
pad=1
batch_normalize=1
activation=leaky

# dw
[route]
layers = -1
group_id=0
groups=3

[convolutional]
batch_normalize=1
filters=48
groups=12
size=3
stride=2
pad=1
activation=leaky

[route]
layers = -3
group_id=1
groups=3

[convolutional]
batch_normalize=1
filters=48
groups=12
size=5
stride=2
pad=1
activation=leaky

[route]
layers = -5
group_id=2
groups=3

[convolutional]
batch_normalize=1
filters=48
groups=12
size=7
stride=2
pad=1
activation=leaky

[route]
layers = -1,-3,-5

# out
[convolutional]
filters=32
groups=2
size=1
stride=1
pad=1
batch_normalize=1
activation=linear



######################
###### 'r1_k3_a1.1_p1.1_s11_e3_i32_o32',
###### 3x3 - 32
###### expand 3 = 96

# expand
[convolutional]
filters=96
groups=2
size=1
stride=1
pad=1
batch_normalize=1
activation=leaky

# dw
[convolutional]
filters=96
groups=24
size=3
stride=1
pad=1
batch_normalize=1
activation=leaky

# out
[convolutional]
filters=32
groups=2
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

[shortcut]
from=-4
activation=linear


######################
###### 'r1_k3.5.7.9_a1_p1_s22_e6_i32_o40_se0.5_sw',
###### 3x3, 5x5, 7x7, 9x9 - 40 - downsample
###### expand 6 = 192

# expand
[convolutional]
filters=192
size=1
stride=1
pad=1
batch_normalize=1
activation=swish

# dw
[route]
layers = -1
group_id=0
groups=4

[convolutional]
batch_normalize=1
filters=48
groups=12
size=3
stride=2
pad=1
activation=swish

[route]
layers = -3
group_id=1
groups=4

[convolutional]
batch_normalize=1
filters=48
groups=12
size=5
stride=2
pad=1
activation=swish

[route]
layers = -5
group_id=2
groups=4

[convolutional]
batch_normalize=1
filters=48
groups=12
size=7
stride=2
pad=1
activation=swish

[route]
layers = -7
group_id=3
groups=4

[convolutional]
batch_normalize=1
filters=48
groups=48
size=9
stride=2
pad=1
activation=swish

[route]
layers = -1,-3,-5,-7


#squeeze-n-excitation
[avgpool]

# squeeze ratio 0.5
[convolutional]
filters=96
size=1
stride=1
activation=swish

# excitation
[convolutional]
filters=192
size=1
stride=1
activation=logistic

# multiply channels
[scale_channels]
from=-4

# out
[convolutional]
filters=40
size=1
stride=1
pad=1
batch_normalize=1
activation=linear



# R1
######################
###### 'r3_k3.5_a1.1_p1.1_s11_e6_i40_o40_se0.5_sw',
###### 3x3, 5x5 - 40
###### expand 6 = 240

# expand
[convolutional]
filters=240
groups=2
size=1
stride=1
pad=1
batch_normalize=1
activation=swish

# dw
[route]
layers = -1
group_id=0
groups=2

[convolutional]
batch_normalize=1
filters=120
groups=30
size=3
stride=1
pad=1
activation=swish

[route]
layers = -3
group_id=1
groups=2

[convolutional]
batch_normalize=1
filters=120
groups=30
size=5
stride=1
pad=1
activation=swish

[route]
layers = -1,-3

#squeeze-n-excitation
[avgpool]

# squeeze ratio 0.5
[convolutional]
filters=120
size=1
stride=1
activation=swish

# excitation
[convolutional]
filters=240
size=1
stride=1
activation=logistic

# multiply channels
[scale_channels]
from=-4

# out
[convolutional]
filters=40
groups=2
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

[shortcut]
from=-12
activation=linear


# R2
######################
###### 'r3_k3.5_a1.1_p1.1_s11_e6_i40_o40_se0.5_sw',
###### 3x3, 5x5 - 40
###### expand 6 = 240

# expand
[convolutional]
filters=240
groups=2
size=1
stride=1
pad=1
batch_normalize=1
activation=swish

# dw
[route]
layers = -1
group_id=0
groups=2

[convolutional]
batch_normalize=1
filters=120
groups=30
size=3
stride=1
pad=1
activation=swish

[route]
layers = -3
group_id=1
groups=2

[convolutional]
batch_normalize=1
filters=120
groups=30
size=5
stride=1
pad=1
activation=swish

[route]
layers = -1,-3

#squeeze-n-excitation
[avgpool]

# squeeze ratio 0.5
[convolutional]
filters=120
size=1
stride=1
activation=swish

# excitation
[convolutional]
filters=240
size=1
stride=1
activation=logistic

# multiply channels
[scale_channels]
from=-4

# out
[convolutional]
filters=40
groups=2
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

[shortcut]
from=-12
activation=linear


# R3
######################
###### 'r3_k3.5_a1.1_p1.1_s11_e6_i40_o40_se0.5_sw',
###### 3x3, 5x5 - 40
###### expand 6 = 240

# expand
[convolutional]
filters=240
groups=2
size=1
stride=1
pad=1
batch_normalize=1
activation=swish

# dw
[route]
layers = -1
group_id=0
groups=2

[convolutional]
batch_normalize=1
filters=120
groups=30
size=3
stride=1
pad=1
activation=swish

[route]
layers = -3
group_id=1
groups=2

[convolutional]
batch_normalize=1
filters=120
groups=30
size=5
stride=1
pad=1
activation=swish

[route]
layers = -1,-3

#squeeze-n-excitation
[avgpool]

# squeeze ratio 0.5
[convolutional]
filters=120
size=1
stride=1
activation=swish

# excitation
[convolutional]
filters=240
size=1
stride=1
activation=logistic

# multiply channels
[scale_channels]
from=-4

# out
[convolutional]
filters=40
groups=2
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

[shortcut]
from=-12
activation=linear


######################
###### 'r1_k3.5.7_a1_p1_s22_e6_i40_o80_se0.25_sw',
###### 3x3, 5x5, 7x7 - 80 - downsample
###### expand 6 = 240

# expand
[convolutional]
filters=240
size=1
stride=1
pad=1
batch_normalize=1
activation=swish

# dw
[route]
layers = -1
group_id=0
groups=3

[convolutional]
batch_normalize=1
filters=80
groups=20
size=3
stride=2
pad=1
activation=swish

[route]
layers = -3
group_id=1
groups=3

[convolutional]
batch_normalize=1
filters=80
groups=20
size=5
stride=2
pad=1
activation=swish

[route]
layers = -5
group_id=2
groups=3

[convolutional]
batch_normalize=1
filters=80
groups=20
size=7
stride=2
pad=1
activation=swish

[route]
layers = -1,-3,-5

#squeeze-n-excitation
[avgpool]

# squeeze ratio 0.25
[convolutional]
filters=60
size=1
stride=1
activation=swish

# excitation
[convolutional]
filters=240
size=1
stride=1
activation=logistic

# multiply channels
[scale_channels]
from=-4

# out
[convolutional]
filters=80
size=1
stride=1
pad=1
batch_normalize=1
activation=linear



# R1
######################
###### 'r3_k3.5.7.9_a1.1_p1.1_s11_e6_i80_o80_se0.25_sw',
###### 3x3, 5x5, 7x7, 9x9 - 80
###### expand 6 = 480

# expand
[convolutional]
filters=480
groups=2
size=1
stride=1
pad=1
batch_normalize=1
activation=swish

# dw
[route]
layers = -1
group_id=0
groups=4

[convolutional]
batch_normalize=1
filters=120
groups=30
size=3
stride=1
pad=1
activation=swish

[route]
layers = -3
group_id=1
groups=4

[convolutional]
batch_normalize=1
filters=120
groups=30
size=5
stride=1
pad=1
activation=swish

[route]
layers = -5
group_id=2
groups=4

[convolutional]
batch_normalize=1
filters=120
groups=30
size=7
stride=1
pad=1
activation=swish

[route]
layers = -7
group_id=3
groups=4

[convolutional]
batch_normalize=1
filters=120
groups=120
size=9
stride=1
pad=1
activation=swish

[route]
layers = -1,-3,-5,-7

#squeeze-n-excitation
[avgpool]

# squeeze ratio 0.25
[convolutional]
filters=120
size=1
stride=1
activation=swish

# excitation
[convolutional]
filters=480
size=1
stride=1
activation=logistic

# multiply channels
[scale_channels]
from=-4

# out
[convolutional]
filters=80
groups=2
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

[shortcut]
from=-16
activation=linear


# R2
######################
###### 'r3_k3.5.7.9_a1.1_p1.1_s11_e6_i80_o80_se0.25_sw',
###### 3x3, 5x5, 7x7, 9x9 - 80
###### expand 6 = 480

# expand
[convolutional]
filters=480
groups=2
size=1
stride=1
pad=1
batch_normalize=1
activation=swish

# dw
[route]
layers = -1
group_id=0
groups=4

[convolutional]
batch_normalize=1
filters=120
groups=30
size=3
stride=1
pad=1
activation=swish

[route]
layers = -3
group_id=1
groups=4

[convolutional]
batch_normalize=1
filters=120
groups=30
size=5
stride=1
pad=1
activation=swish

[route]
layers = -5
group_id=2
groups=4

[convolutional]
batch_normalize=1
filters=120
groups=30
size=7
stride=1
pad=1
activation=swish

[route]
layers = -7
group_id=3
groups=4

[convolutional]
batch_normalize=1
filters=120
groups=120
size=9
stride=1
pad=1
activation=swish

[route]
layers = -1,-3,-5,-7

#squeeze-n-excitation
[avgpool]

# squeeze ratio 0.25
[convolutional]
filters=120
size=1
stride=1
activation=swish

# excitation
[convolutional]
filters=480
size=1
stride=1
activation=logistic

# multiply channels
[scale_channels]
from=-4

# out
[convolutional]
filters=80
groups=2
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

[shortcut]
from=-16
activation=linear


# R3
######################
###### 'r3_k3.5.7.9_a1.1_p1.1_s11_e6_i80_o80_se0.25_sw',
###### 3x3, 5x5, 7x7, 9x9 - 80
###### expand 6 = 480

# expand
[convolutional]
filters=480
groups=2
size=1
stride=1
pad=1
batch_normalize=1
activation=swish

# dw
[route]
layers = -1
group_id=0
groups=4

[convolutional]
batch_normalize=1
filters=120
groups=30
size=3
stride=1
pad=1
activation=swish

[route]
layers = -3
group_id=1
groups=4

[convolutional]
batch_normalize=1
filters=120
groups=30
size=5
stride=1
pad=1
activation=swish

[route]
layers = -5
group_id=2
groups=4

[convolutional]
batch_normalize=1
filters=120
groups=30
size=7
stride=1
pad=1
activation=swish

[route]
layers = -7
group_id=3
groups=4

[convolutional]
batch_normalize=1
filters=120
groups=120
size=9
stride=1
pad=1
activation=swish

[route]
layers = -1,-3,-5,-7

#squeeze-n-excitation
[avgpool]

# squeeze ratio 0.25
[convolutional]
filters=120
size=1
stride=1
activation=swish

# excitation
[convolutional]
filters=480
size=1
stride=1
activation=logistic

# multiply channels
[scale_channels]
from=-4

# out
[convolutional]
filters=80
groups=2
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

[shortcut]
from=-16
activation=linear


######################
###### 'r1_k3_a1_p1_s11_e6_i80_o120_se0.5_sw',
###### 3x3 - 120
###### expand 6 = 480

# expand
[convolutional]
filters=480
size=1
stride=1
pad=1
batch_normalize=1
activation=swish

# dw
[convolutional]
filters=480
groups=480
size=3
stride=1
pad=1
batch_normalize=1
activation=swish

#squeeze-n-excitation
[avgpool]

# squeeze ratio 0.5
[convolutional]
filters=240
size=1
stride=1
activation=swish

# excitation
[convolutional]
filters=480
size=1
stride=1
activation=logistic

# multiply channels
[scale_channels]
from=-4

# out
[convolutional]
filters=120
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

[shortcut]
from=-8
activation=linear


# R1
######################
###### 'r3_k3.5.7.9_a1.1_p1.1_s11_e3_i120_o120_se0.5_sw',
###### 3x3, 5x5, 7x7, 9x9 - 120
###### expand 6 = 720

# expand
[convolutional]
filters=720
groups=2
size=1
stride=1
pad=1
batch_normalize=1
activation=swish

# dw
[route]
layers = -1
group_id=0
groups=4

[convolutional]
batch_normalize=1
filters=180
groups=45
size=3
stride=1
pad=1
activation=swish

[route]
layers = -3
group_id=1
groups=4

[convolutional]
batch_normalize=1
filters=180
groups=45
size=5
stride=1
pad=1
activation=swish

[route]
layers = -5
group_id=2
groups=4

[convolutional]
batch_normalize=1
filters=180
groups=45
size=7
stride=1
pad=1
activation=swish

[route]
layers = -7
group_id=3
groups=4

[convolutional]
batch_normalize=1
filters=180
groups=180
size=9
stride=1
pad=1
activation=swish

[route]
layers = -1,-3,-5,-7

#squeeze-n-excitation
[avgpool]

# squeeze ratio 0.5
[convolutional]
filters=360
size=1
stride=1
activation=swish

# excitation
[convolutional]
filters=720
size=1
stride=1
activation=logistic

# multiply channels
[scale_channels]
from=-4

# out
[convolutional]
filters=120
groups=2
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

[shortcut]
from=-16
activation=linear


# R2
######################
###### 'r3_k3.5.7.9_a1.1_p1.1_s11_e3_i120_o120_se0.5_sw',
###### 3x3, 5x5, 7x7, 9x9 - 120
###### expand 6 = 720

# expand
[convolutional]
filters=720
groups=2
size=1
stride=1
pad=1
batch_normalize=1
activation=swish

# dw
[route]
layers = -1
group_id=0
groups=4

[convolutional]
batch_normalize=1
filters=180
groups=45
size=3
stride=1
pad=1
activation=swish

[route]
layers = -3
group_id=1
groups=4

[convolutional]
batch_normalize=1
filters=180
groups=45
size=5
stride=1
pad=1
activation=swish

[route]
layers = -5
group_id=2
groups=4

[convolutional]
batch_normalize=1
filters=180
groups=45
size=7
stride=1
pad=1
activation=swish

[route]
layers = -7
group_id=3
groups=4

[convolutional]
batch_normalize=1
filters=180
groups=180
size=9
stride=1
pad=1
activation=swish

[route]
layers = -1,-3,-5,-7

#squeeze-n-excitation
[avgpool]

# squeeze ratio 0.5
[convolutional]
filters=360
size=1
stride=1
activation=swish

# excitation
[convolutional]
filters=720
size=1
stride=1
activation=logistic

# multiply channels
[scale_channels]
from=-4

# out
[convolutional]
filters=120
groups=2
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

[shortcut]
from=-16
activation=linear


# R3
######################
###### 'r3_k3.5.7.9_a1.1_p1.1_s11_e3_i120_o120_se0.5_sw',
###### 3x3, 5x5, 7x7, 9x9 - 120
###### expand 6 = 720

# expand
[convolutional]
filters=720
groups=2
size=1
stride=1
pad=1
batch_normalize=1
activation=swish

# dw
[route]
layers = -1
group_id=0
groups=4

[convolutional]
batch_normalize=1
filters=180
groups=45
size=3
stride=1
pad=1
activation=swish

[route]
layers = -3
group_id=1
groups=4

[convolutional]
batch_normalize=1
filters=180
groups=45
size=5
stride=1
pad=1
activation=swish

[route]
layers = -5
group_id=2
groups=4

[convolutional]
batch_normalize=1
filters=180
groups=45
size=7
stride=1
pad=1
activation=swish

[route]
layers = -7
group_id=3
groups=4

[convolutional]
batch_normalize=1
filters=180
groups=180
size=9
stride=1
pad=1
activation=swish

[route]
layers = -1,-3,-5,-7

#squeeze-n-excitation
[avgpool]

# squeeze ratio 0.5
[convolutional]
filters=360
size=1
stride=1
activation=swish

# excitation
[convolutional]
filters=720
size=1
stride=1
activation=logistic

# multiply channels
[scale_channels]
from=-4

# out
[convolutional]
filters=120
groups=2
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

[shortcut]
from=-16
activation=linear


######################
###### 'r1_k3.5.7.9_a1_p1_s22_e6_i120_o200_se0.5_sw',
###### 3x3, 5x5, 7x7, 9x9 - 200 - downsample
###### expand 6 = 720

# expand
[convolutional]
filters=720
size=1
stride=1
pad=1
batch_normalize=1
activation=swish

# dw
[route]
layers = -1
group_id=0
groups=4

[convolutional]
batch_normalize=1
filters=180
groups=45
size=3
stride=2
pad=1
activation=swish

[route]
layers = -3
group_id=1
groups=4

[convolutional]
batch_normalize=1
filters=180
groups=45
size=5
stride=2
pad=1
activation=swish

[route]
layers = -5
group_id=2
groups=4

[convolutional]
batch_normalize=1
filters=180
groups=45
size=7
stride=2
pad=1
activation=swish

[route]
layers = -7
group_id=3
groups=4

[convolutional]
batch_normalize=1
filters=180
groups=180
size=9
stride=2
pad=1
activation=swish

[route]
layers = -1,-3,-5,-7


#squeeze-n-excitation
[avgpool]

# squeeze ratio 0.5
[convolutional]
filters=360
size=1
stride=1
activation=swish

# excitation
[convolutional]
filters=720
size=1
stride=1
activation=logistic

# multiply channels
[scale_channels]
from=-4

# out
[convolutional]
filters=200
size=1
stride=1
pad=1
batch_normalize=1
activation=linear



# R1
######################
###### 'r3_k3.5.7.9_a1_p1.1_s11_e6_i200_o200_se0.5_sw',
###### 3x3, 5x5, 7x7, 9x9 - 200
###### expand 6 = 1200

# expand
[convolutional]
filters=1200
size=1
stride=1
pad=1
batch_normalize=1
activation=swish

# dw
[route]
layers = -1
group_id=0
groups=4

[convolutional]
batch_normalize=1
filters=300
groups=75
size=3
stride=1
pad=1
activation=swish

[route]
layers = -3
group_id=1
groups=4

[convolutional]
batch_normalize=1
filters=300
groups=75
size=5
stride=1
pad=1
activation=swish

[route]
layers = -5
group_id=2
groups=4

[convolutional]
batch_normalize=1
filters=300
groups=75
size=7
stride=1
pad=1
activation=swish

[route]
layers = -7
group_id=3
groups=4

[convolutional]
batch_normalize=1
filters=300
groups=300
size=9
stride=1
pad=1
activation=swish

[route]
layers = -1,-3,-5,-7


#squeeze-n-excitation
[avgpool]

# squeeze ratio 0.5
[convolutional]
filters=600
size=1
stride=1
activation=swish

# excitation
[convolutional]
filters=1200
size=1
stride=1
activation=logistic

# multiply channels
[scale_channels]
from=-4

# out
[convolutional]
filters=200
groups=2
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

[shortcut]
from=-16
activation=linear


# R2
######################
###### 'r3_k3.5.7.9_a1_p1.1_s11_e6_i200_o200_se0.5_sw',
###### 3x3, 5x5, 7x7, 9x9 - 200
###### expand 6 = 1200

# expand
[convolutional]
filters=1200
size=1
stride=1
pad=1
batch_normalize=1
activation=swish

# dw
[route]
layers = -1
group_id=0
groups=4

[convolutional]
batch_normalize=1
filters=300
groups=75
size=3
stride=1
pad=1
activation=swish

[route]
layers = -3
group_id=1
groups=4

[convolutional]
batch_normalize=1
filters=300
groups=75
size=5
stride=1
pad=1
activation=swish

[route]
layers = -5
group_id=2
groups=4

[convolutional]
batch_normalize=1
filters=300
groups=75
size=7
stride=1
pad=1
activation=swish

[route]
layers = -7
group_id=3
groups=4

[convolutional]
batch_normalize=1
filters=300
groups=300
size=9
stride=1
pad=1
activation=swish

[route]
layers = -1,-3,-5,-7

#squeeze-n-excitation
[avgpool]

# squeeze ratio 0.5
[convolutional]
filters=600
size=1
stride=1
activation=swish

# excitation
[convolutional]
filters=1200
size=1
stride=1
activation=logistic

# multiply channels
[scale_channels]
from=-4

# out
[convolutional]
filters=200
groups=2
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

[shortcut]
from=-16
activation=linear


# R3
######################
###### 'r3_k3.5.7.9_a1_p1.1_s11_e6_i200_o200_se0.5_sw',
###### 3x3, 5x5, 7x7, 9x9 - 200
###### expand 6 = 1200

# expand
[convolutional]
filters=1200
size=1
stride=1
pad=1
batch_normalize=1
activation=swish

# dw
[route]
layers = -1
group_id=0
groups=4

[convolutional]
batch_normalize=1
filters=300
groups=75
size=3
stride=1
pad=1
activation=swish

[route]
layers = -3
group_id=1
groups=4

[convolutional]
batch_normalize=1
filters=300
groups=75
size=5
stride=1
pad=1
activation=swish

[route]
layers = -5
group_id=2
groups=4

[convolutional]
batch_normalize=1
filters=300
groups=75
size=7
stride=1
pad=1
activation=swish

[route]
layers = -7
group_id=3
groups=4

[convolutional]
batch_normalize=1
filters=300
groups=300
size=9
stride=1
pad=1
activation=swish

[route]
layers = -1,-3,-5,-7

#squeeze-n-excitation
[avgpool]

# squeeze ratio 0.5
[convolutional]
filters=600
size=1
stride=1
activation=swish

# excitation
[convolutional]
filters=1200
size=1
stride=1
activation=logistic

# multiply channels
[scale_channels]
from=-4

# out
[convolutional]
filters=200
groups=2
size=1
stride=1
pad=1
batch_normalize=1
activation=linear

[shortcut]
from=-16
activation=linear


# _conv_head
[convolutional]
filters=1536
size=1
stride=1
pad=1
batch_normalize=1
activation=swish


[avgpool]

[dropout]
probability=.25

[convolutional]
filters=1000
size=1
stride=1
pad=0
activation=linear

[softmax]
groups=1

#[cost]
#type=sse

